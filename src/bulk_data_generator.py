"""
BÃ¼yÃ¼k Veri Seti Ãœretimi - Spor Yetenek Tahmin Sistemi
Bu dosya, bÃ¼yÃ¼k veri setlerini parÃ§alar halinde Ã¼retir.
"""

import pandas as pd
import numpy as np
from data_generator import SportsDataGenerator
import os
from typing import List
import time

class BulkDataGenerator:
    """BÃ¼yÃ¼k veri setlerini parÃ§alar halinde Ã¼retir"""
    
    def __init__(self, batch_size: int = 50, seed: int = 42):
        """
        Bulk veri Ã¼retici sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r
        
        Args:
            batch_size: Her seferinde Ã¼retilecek kiÅŸi sayÄ±sÄ±
            seed: Rastgele sayÄ± Ã¼reteci iÃ§in seed deÄŸeri
        """
        self.batch_size = batch_size
        self.base_seed = seed
        self.generator = SportsDataGenerator(seed=seed)
        
    def generate_batch(self, batch_num: int) -> pd.DataFrame:
        """Tek bir batch veri Ã¼retir"""
        # Her batch iÃ§in farklÄ± seed kullan
        batch_seed = self.base_seed + batch_num * 1000
        self.generator = SportsDataGenerator(seed=batch_seed)
        
        return self.generator.generate_dataset(self.batch_size)
    
    def generate_large_dataset(self, total_size: int, 
                             save_path: str = "data/sporcu_dataset_large.csv",
                             progress_callback=None) -> pd.DataFrame:
        """
        BÃ¼yÃ¼k veri setini parÃ§alar halinde Ã¼retir
        
        Args:
            total_size: Toplam kiÅŸi sayÄ±sÄ±
            save_path: Kaydedilecek dosya yolu
            progress_callback: Ä°lerleme callback fonksiyonu
        """
        print(f"ğŸš€ {total_size} kiÅŸilik veri seti Ã¼retiliyor...")
        print(f"ğŸ“¦ Batch boyutu: {self.batch_size}")
        
        all_data = []
        batches = (total_size + self.batch_size - 1) // self.batch_size
        
        for batch_num in range(batches):
            start_time = time.time()
            
            # Son batch iÃ§in kalan kiÅŸi sayÄ±sÄ±nÄ± hesapla
            remaining = total_size - (batch_num * self.batch_size)
            current_batch_size = min(self.batch_size, remaining)
            
            # Batch Ã¼ret
            if current_batch_size > 0:
                # Bu batch iÃ§in Ã¶zel generator
                batch_generator = SportsDataGenerator(seed=self.base_seed + batch_num * 1000)
                batch_data = batch_generator.generate_dataset(current_batch_size)
                all_data.append(batch_data)
            
            # Ä°lerleme bilgisi
            completed = (batch_num + 1) * self.batch_size
            if completed > total_size:
                completed = total_size
                
            elapsed_time = time.time() - start_time
            
            print(f"âœ… Batch {batch_num + 1}/{batches} tamamlandÄ±: "
                  f"{completed}/{total_size} kiÅŸi ({elapsed_time:.2f}s)")
            
            if progress_callback:
                progress_callback(completed, total_size, batch_num + 1, batches)
        
        # TÃ¼m batch'leri birleÅŸtir
        print("ğŸ”„ Batch'ler birleÅŸtiriliyor...")
        final_dataset = pd.concat(all_data, ignore_index=True)
        
        # Dosyaya kaydet
        print(f"ğŸ’¾ Veri seti kaydediliyor: {save_path}")
        final_dataset.to_csv(save_path, index=False, encoding='utf-8')
        
        # Ã–zet bilgiler
        print("\n" + "="*50)
        print("ğŸ“Š BÃœYÃœK VERÄ° SETÄ° Ã–ZET BÄ°LGÄ°LERÄ°")
        print("="*50)
        print(f"ğŸ“ˆ Toplam kiÅŸi sayÄ±sÄ±: {len(final_dataset)}")
        print(f"ğŸ“Š Ã–zellik sayÄ±sÄ±: {len(final_dataset.columns)}")
        print(f"ğŸ“ Dosya boyutu: {os.path.getsize(save_path) / (1024*1024):.2f} MB")
        print(f"ğŸ“‹ Ortalama yaÅŸ: {final_dataset['yas'].mean():.1f}")
        
        print(f"\nğŸ† En Ã§ok tavsiye edilen sporlar (TÃ¼rkiye'ye Ã¶zel):")
        sport_counts = final_dataset['tavsiye_edilen_spor'].value_counts()
        for i, (sport, count) in enumerate(sport_counts.head(5).items(), 1):
            print(f"   {i}. {sport}: {count} kiÅŸi (%{count/len(final_dataset)*100:.1f})")
        
        print(f"\nğŸ‘¥ Cinsiyet daÄŸÄ±lÄ±mÄ±:")
        gender_counts = final_dataset['cinsiyet'].value_counts()
        for gender, count in gender_counts.items():
            print(f"   {gender}: {count} kiÅŸi (%{count/len(final_dataset)*100:.1f})")
        
        return final_dataset
    
    def generate_progressive_dataset(self, size_list: List[int],
                                   base_path: str = "data/sporcu_dataset"):
        """
        FarklÄ± boyutlarda veri setleri Ã¼retir
        
        Args:
            size_list: Ãœretilecek boyutlar listesi [100, 200, 500]
            base_path: Temel dosya yolu
        """
        for size in size_list:
            print(f"\nğŸ¯ {size} kiÅŸilik veri seti Ã¼retiliyor...")
            
            dataset = self.generate_large_dataset(
                total_size=size,
                save_path=f"{base_path}_{size}.csv"
            )
            
            print(f"âœ… {size} kiÅŸilik veri seti tamamlandÄ±!\n")

# KullanÄ±m fonksiyonlarÄ±
def generate_500_dataset():
    """500 kiÅŸilik veri seti Ã¼retir"""
    bulk_generator = BulkDataGenerator(batch_size=50, seed=42)
    return bulk_generator.generate_large_dataset(
        total_size=500,
        save_path="data/sporcu_dataset_500.csv"
    )

def generate_multiple_datasets():
    """FarklÄ± boyutlarda veri setleri Ã¼retir"""
    bulk_generator = BulkDataGenerator(batch_size=50, seed=42)
    bulk_generator.generate_progressive_dataset(
        size_list=[200, 300, 500],
        base_path="data/sporcu_dataset"
    )

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    print("ğŸ‡¹ğŸ‡· Spor Yetenek Tahmin Sistemi - TÃ¼rkiye'ye Ã–zel BÃ¼yÃ¼k Veri Seti Ãœretimi")
    print("="*70)
    
    # 500 kiÅŸilik TÃ¼rkiye'ye Ã¶zel veri seti Ã¼ret
    generate_500_dataset()
    
    print("\nğŸ‰ TÃ¼rkiye'ye Ã¶zel veri seti Ã¼retimi tamamlandÄ±!")
    print("ğŸ“ Dosya: data/sporcu_dataset_500.csv")
    print("ğŸ‡¹ğŸ‡· Ã–zellikler: TÃ¼rk insanlarÄ±nÄ±n fiziksel Ã¶zellikleri, popÃ¼ler sporlar, coÄŸrafi bÃ¶lgeler")
    print("ğŸŒ Streamlit uygulamasÄ±nÄ± yeniden baÅŸlatÄ±n") 